{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "horizontal-melissa",
   "metadata": {},
   "source": [
    "Based on [first part of Speech processing tutorial series](https://towardsdatascience.com/audio-deep-learning-made-simple-part-1-state-of-the-art-techniques-da1d3dff2504/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "demanding-invite",
   "metadata": {},
   "source": [
    "- A sound signal is produced by variations in air pressure. Sound signals often repeat at regular intervals so that each wave has the same shape. The height shows the intensity of the sound and is known as the amplitude. The time taken for the signal to complete one full wave is the period. The number of waves made by the signal in one second is called the frequency. The frequency is the reciprocal of the period. <br>\n",
    "<img src=\"./images/1-sound_signal.png\">\n",
    "- The majority of sounds may not follow such simple and regular periodic patterns. But signals of different frequencies can be added together to create composite signals with more complex repeating patterns. All sounds that we hear, including our own human voice, consist of waveforms like these. <br>\n",
    "<img src=\"./images/2-sound_components.png\">\n",
    "- To **digitize** a sound wave, we must turn the signal into a series of numbers so that we can input it into out models. This is done by measuring the amplitude of the sound at fixed intervals of time. *Each such measurement is called a **sample**, and the sample rate is the number of samples per second. For instance, a common sampling rate is about 44,100 samples per second. That means a 10-second music clip would have 441,000 samples.* <br>\n",
    "<img src=\"./images/3-sound_sampling.png\">\n",
    "- Input for deep learning methods is the \"image\" of audio file. This is done by generating Spectrograms from the audio. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "internal-broad",
   "metadata": {},
   "source": [
    "### Spectrogram\n",
    "#### Spectrum\n",
    "- Signals of different frequencies can be added together to create composite signals, representing any sound that occurs in the real-world. This means that any signal consists of many distinct frequencies and can be expressed as the sum of those frequencies. <br>\n",
    "- **The Spectrum** is the set of frequencies that are combined together to produce a signal. **The Spectrum** plots all of the frequencies that are present in the signal along with the strength or amplitude of each frequency.\n",
    "<img src=\"./images/4-spectrum.png\">\n",
    "- The lowest frequency in a signal is called **the fundamental frequency**. Frequencies that are whole number multiples of the fundamental frequency are known as harmonics. <br>\n",
    "    * For instance, if the fundamental frequency is 200 Hz, then its harmonic frequencies are 400 Hz, 600 Hz, and so on. <br>\n",
    "    \n",
    "#### Time Domain and Frequency Domain\n",
    "- The waveforms that we saw earlier showing Amplitude against Time are one way to represent a sound signal. Since the x-axis shows the range of time values of the signal, we are viewing the signal in the Time domain.\n",
    "- Spectrum is an alternate way to represent the same signal. It shows Amplitude against Frequency, and since the x-axis shows the range of frequency values of the signal, at a moment in time, we are viewing the signal in the Frequency Domain.\n",
    "<img src=\"./images/5-time_and_frequency_domains.png\">\n",
    "\n",
    "#### Spectrograms\n",
    "- Since a signal produces different sounds as it varies over time, its constituent frequencies also vary with time. In other words, its **Spectrum** varies with time.\n",
    "- **A Spectrogram** of a signal plots its **Spectrum** over time and is like a \"photograph\" of the signal. It plots Time on the x-axis and Frequency on the y-axis. It is as though we though the **Spectrum** again ans again at different instances in time, and then joined them all together into a single plot.\n",
    "- It uses different colors to indicate the Amplitude or strength of each frequency. The brighter the color, the higher the energy of the signal. Each vertical \"slice\" of the Spectrogram is essentially the Spectrum of the signal at that instant in time and shows how the signal strength is distributed in every frequency found in the signal at that instant.\n",
    "- In the following figure, the first picture displays the signal in the Time domain ie. Amplitude vs Time. It gives us a sense of how loud or quiet a clip  is at any point in time, but it gives us a very little information about which frequencies are present. The second picture is the **Spectrogram** and displays the signal in the Frequency domain.\n",
    "<img src=\"./images/6-spectrogram.png\">\n",
    "\n",
    "#### Generating Spectrograms\n",
    "- Spectrograms are produced using **Fourier Transforms** to decompose any signal into its constituent frequencies."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "radical-genetics",
   "metadata": {},
   "source": [
    "### Audio Deep Learning Models\n",
    "<img src=\"./images/7-audio_DL.png\">\n",
    "- Most deep learning audio applications use **Spectrograms** to represent audio. They follow a procedure like this:\n",
    "\n",
    "    1. Start with raw audio data in the form of a wave file\n",
    "    2. Convert the audio data into its corresponding spectrogram\n",
    "    3. (Optional) Use simple audio processing technique to augment the spectrogram data. (Some augmentation or cleaning can also be done on the raw audio before the spectrogram conversion)\n",
    "    4. Apply standard CNN architectures to process and extract feature maps that are an encoded representation of the spectrogram image\n",
    "    5. The next step is to generate output predictions from this encoded representation:\n",
    "        1. Classification\n",
    "        2. Text-to-speech: pass  through some RNN layers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "extended-mistress",
   "metadata": {},
   "source": [
    "### What problems does audio deep learning solve?\n",
    "#### Audio classification\n",
    "<img src=\"./images/8-audio_classification.png\">\n",
    "- This could be applied to detect the failure of machinery or equipment based on the sound that it produces, or in a surveillance system, to detect security break-ins.\n",
    "\n",
    "#### Audio Separation and Segmentation\n",
    "- Audio Separation involves isolating a signal of interest from a mixture of signals so that it can then be used for further processing. For instance, it is to seperate out individual people's voices from a lot of background noise, or the sound of the violin from the rest of the musical performance.\n",
    "<img src=\"./images/9-audio_separation.png\">\n",
    "\n",
    "#### Music Genre Classification and Tagging\n",
    "- Identify and categorize music based on the audio. The content of the music is analyzed to figure out the genre to which it belongs. This is a multi-label classification problem because a given piece of music might fall under more than one genre (rock, pop, jazz, ...)\n",
    "<img src=\"./images/10-audio_genre.png\">\n",
    "- Speaker information can be tags (ages, genre, ...)\n",
    "\n",
    "#### Music Generation and Music Transcription\n",
    "<img src=\"./images/11-music_generation.png\">\n",
    "\n",
    "#### Voice Recognition\n",
    "<img src=\"./images/12-voice_identification.png\">\n",
    "\n",
    "#### Speech to Text and Text to Speech\n",
    "- This is one of the most challenging applications because it deals not just with analyzing audio, but also with NLP and requires developing some basic language capability to decipher distinct words from the uttered sounds.\n",
    "<img src=\"./images/13-speech_to_text.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "funded-census",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "velvet-trading",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "suited-oriental",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "velvet-approval",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "minus-coaching",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accepting-penny",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "binary-possession",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
